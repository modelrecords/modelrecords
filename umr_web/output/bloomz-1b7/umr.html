
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="generator" content="Pelican" />
    <title>bloomz-1b7 - UMR Card (None) </title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
      href="https://fonts.googleapis.com/css2?family=Roboto+Condensed:ital,wght@0,100..900;1,100..900&family=Roboto:ital,wght@0,400;0,500;0,700;1,400;1,500;1,700&display=swap"
      rel="stylesheet">
    <link rel="stylesheet" href="../theme/css/main.css" />
    <script defer src="https://cdn.jsdelivr.net/npm/alpinejs@3.x.x/dist/cdn.min.js"></script>
</head>

<body>
  <header id="banner" class="bg-primarylight">
    <h1><a href="../">Unified Model Record</a></h1>
    <nav>
      <ul class="flex gap-5">
        <li><a href="/cards.html" class="text-sky-400">All Cards</a></li>
      </ul>
    </nav>
  </header><!-- /#banner -->

<main class="flex grow flex-col">
<header class="bg-highlight p-8 border-b border-black">
  <h1 class="text-4xl font-bold">bloomz-1b7</h1>
    <p class="text-2xl font-medium capitalize">Type: model</p>
</header>
  <div class="flex grow">
    <nav class="flex-none w-48 py-4">

<h3 class="roboto-condensed font-bold text-xs uppercase py-2 pr-2 pl-4">The Cards</h3>
<ul>
  <li><a href="umr.html" class="block roboto-condensed font-bold text-lg uppercase py-2 pr-2 pl-4 bg-highlight">UMR Card</a></li>
  <li><a href="model.html" class="block roboto-condensed font-bold text-lg uppercase py-2 pr-2 pl-4 ">Model Card</a></li>
</ul>    </nav>
    <div class="w-4xl px-8 border-l border-black">
      <div class="flex justify-end items-center roboto-condensed text-xs pb-6">
        <span class="px-2">Publisher: BigScience Workshop</span>
        <span class="px-2">Released: 2022-11-17</span>
        <span class="font-extrabold bg-primarydark p-2 pt-1.5 ml-2 text-white">v.1.0.0</span>
      </div>
      <div class="flex gap-x-8">
        <div class="w-2/3">
  <section class="mb-3.5">
<header class="mb-3">
    <h2 class="uppercase text-primary roboto-condensed font-bold text-lg border-b-2 border-primary mb-1.5">Metadata</h2>
        <p>General information.</p>
</header>    <div class="section-content">
      <dl>
            <dt>name</dt>
            <dd>bloomz-1b7</dd>
            <dt>version</dt>
            <dd>1.0.0</dd>
            <dt>publisher</dt>
            <dd>BigScience Workshop</dd>
            <dt>release date</dt>
            <dd>2022-11-17</dd>
            <dt>model type</dt>
            <dd>Multitask Fine-tuned Language Model</dd>
      </dl>
    </div>
  </section>

  <section class="mb-3.5">
<header class="mb-3">
    <h2 class="uppercase text-primary roboto-condensed font-bold text-lg border-b-2 border-primary mb-1.5">Relations</h2>
</header>    <div class="section-content">
      <ul>
          <li><a href="/bloomz/umr.html">bloomz>=1.0.0</a></li>
      </ul>
    </div>
  </section>

  <section class="mb-3.5">
<header class="mb-3">
    <h2 class="uppercase text-primary roboto-condensed font-bold text-lg border-b-2 border-primary mb-1.5">Intended Use</h2>
</header>    <div class="section-content">
      <ul>
          <li>Natural language processing tasks, including but not limited to translation, sentiment analysis, and question answering.</li>
          <li>Cross-lingual understanding and generation tasks.</li>
          <li>Instruction-based prompt generation for a wide range of languages.</li>
          <li>Zero-shot and few-shot learning applications.</li>
          <li>Exploratory data analysis and research in multilingual language model capabilities.</li>
      </ul>
    </div>
  </section>

  <section class="mb-3.5">
<header class="mb-3">
    <h2 class="uppercase text-primary roboto-condensed font-bold text-lg border-b-2 border-primary mb-1.5">Factors</h2>
</header>    <div class="section-content">
      <ul>
          <li>Language support and proficiency across a broad spectrum of languages.</li>
          <li>The clarity and specificity of instruction prompts.</li>
          <li>Model scalability and performance across different sizes from 300M to 176B parameters.</li>
          <li>Generalization abilities to unseen tasks and languages.</li>
          <li>Accessibility and ease of use for researchers and developers with different levels of resources.</li>
      </ul>
    </div>
  </section>

  <section class="mb-3.5">
<header class="mb-3">
    <h2 class="uppercase text-primary roboto-condensed font-bold text-lg border-b-2 border-primary mb-1.5">Evaluation Data</h2>
</header>    <div class="section-content">
      <ul>
          <li>Description: A diverse set of evaluation tasks covering coreference resolution, natural language inference, sentence completion, and program synthesis across multiple languages.</li>
          <li>Description: Datasets from the Winogrande, ANLI, XNLI, and HumanEval evaluations, allowing for an extensive assessment of model performance in both seen and unseen languages.</li>
          <li>Description: Validation and test splits are utilized from the respective datasets to ensure unbiased evaluation.</li>
          <li>Description: Multilingual task evaluation employing prompts in both English and the respective native languages to gauge cross-lingual transfer capabilities.</li>
          <li>Description: Benchmarking against existing models like XGLM, T0, and GPT to understand the competitive landscape.</li>
      </ul>
    </div>
  </section>

  <section class="mb-3.5">
<header class="mb-3">
    <h2 class="uppercase text-primary roboto-condensed font-bold text-lg border-b-2 border-primary mb-1.5">Training Data</h2>
</header>    <div class="section-content">
      <ul>
          <li>Description: The model utilizes the BIG-bench xP3 dataset for training, promoting a wide coverage of tasks and languages.</li>
          <li>Description: Incorporation of code and programming languages alongside natural languages to enhance the model's versatility.</li>
          <li>Description: Utilized datasets such as BIG-bench, ROOTS, and a subset of the mC4 corpus to provide rich, diverse linguistic and task coverage.</li>
          <li>Description: Finetuning approach on xP3, xP3mt, and P3 datasets to enable cross-lingual generalization and effective prompt-based task performance.</li>
          <li>Description: Leverages both pretrained (BLOOM, mT5) and bespoke large language models across various sizes for targeted task learning.</li>
      </ul>
    </div>
  </section>

  <section class="mb-3.5">
<header class="mb-3">
    <h2 class="uppercase text-primary roboto-condensed font-bold text-lg border-b-2 border-primary mb-1.5">Additional Information</h2>
</header>    <div class="section-content">
      <ul>
          <li>The project is conducted under the BigScience initiative, allowing for open collaboration and research.</li>
          <li>Models are released under RAIL and Apache 2.0 licenses for wide accessibility and use.</li>
          <li>Fine-tuned models incorporate biases towards short answers, affecting performance on generative tasks.</li>
          <li>Language contamination analysis in the pretraining corpus shows unintentional learning from 'unseen' languages.</li>
          <li>Recommendations include using a specific prompting format and considering model size according to task requirements.</li>
      </ul>
    </div>
  </section>

  <section class="mb-3.5">
<header class="mb-3">
    <h2 class="uppercase text-primary roboto-condensed font-bold text-lg border-b-2 border-primary mb-1.5">Recommendations</h2>
</header>    <div class="section-content">
      <ul>
          <li>Employment of early stopping, addition of long tasks, and minimum generation length forcing for improved generative task performance.</li>
          <li>Fine-tuning with both English and machine-translated multilingual prompts for enhanced cross-lingual abilities.</li>
          <li>Utilization of the model in research to explore and expand the boundaries of zero-shot learning across languages.</li>
          <li>Adoption of ethical and fair use practices, considering the model's broad linguistic capabilities.</li>
          <li>Engagement with the BigScience community for collaborative research and development efforts.</li>
      </ul>
    </div>
  </section>
        </div>
        <div class="w-1/3">
  <div>
    right
  </div>
        </div>
      </div>
    </div>
  </div>
</main>

  <section id="extras" class="hidden">
    <div class="social">
      <h2>social</h2>
      <ul>
        <li><a href="#">You can add links in your config file</a></li>
        <li><a href="#">Another social link</a></li>
      </ul>
    </div><!-- /.social -->
  </section><!-- /#extras -->

  <footer id="contentinfo" class="body">
  </footer><!-- /#contentinfo -->
</body>
</html>