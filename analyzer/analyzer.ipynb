{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import openai\n",
    "from llama_index.core import (\n",
    "    Document,\n",
    "    load_index_from_storage,\n",
    "    Settings,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    VectorStoreIndex,\n",
    ")\n",
    "from llama_index.core.node_parser import (\n",
    "    get_leaf_nodes,\n",
    "    HierarchicalNodeParser,\n",
    "    SentenceWindowNodeParser,\n",
    ")\n",
    "from llama_index.core.indices.postprocessor import (\n",
    "    MetadataReplacementPostProcessor,\n",
    "    SentenceTransformerRerank,\n",
    ")\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.retrievers import AutoMergingRetriever\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build or load the sentence window index.\n",
    "def build_sentence_window_index(\n",
    "    file_path,\n",
    "    api_key,\n",
    "    model=\"gpt-4o\",\n",
    "    embedding_model=\"text-embedding-3-large\",\n",
    "    index_dir=\"./sentence_index\",\n",
    "):\n",
    "    # Load and parse the document\n",
    "    documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
    "    document_text = \"\\n\\n\".join([doc.text for doc in documents])\n",
    "    document = Document(text=document_text)\n",
    "\n",
    "    # Segment the document using a Sentence Window Parser\n",
    "    node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "        window_size=4,\n",
    "        window_metadata_key=\"window\",\n",
    "        original_text_metadata_key=\"original_sentence\",\n",
    "    )\n",
    "\n",
    "    # Set up OpenAI integration and embeddings\n",
    "    openai.api_key = api_key\n",
    "    llm = OpenAI(model=model, temperature=0.1, api_key=api_key)\n",
    "    embedding = OpenAIEmbedding(model=embedding_model)\n",
    "\n",
    "    # Create the index\n",
    "    Settings.llm = llm\n",
    "    Settings.embeddings = embedding\n",
    "    Settings.node_parser = node_parser\n",
    "\n",
    "    if not os.path.exists(index_dir):\n",
    "        sentence_index = VectorStoreIndex.from_documents(\n",
    "            [document], service_context=Settings\n",
    "        )\n",
    "        sentence_index.storage_context.persist(persist_dir=index_dir)\n",
    "    else:\n",
    "        sentence_index = load_index_from_storage(\n",
    "            StorageContext.from_defaults(persist_dir=index_dir),\n",
    "            service_context=Settings,\n",
    "        )\n",
    "\n",
    "    return sentence_index\n",
    "\n",
    "\n",
    "# Function to get the sentence window query engine\n",
    "def get_sentence_window_query_engine(sentence_index):\n",
    "    # Set up post-processing\n",
    "    postproc = MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "\n",
    "    # Set up the query engine\n",
    "    sentence_window_engine = sentence_index.as_query_engine(\n",
    "        similarity_top_k=10, node_postprocessors=[postproc]\n",
    "    )\n",
    "\n",
    "    return sentence_window_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_automerging_index(\n",
    "    file_path,\n",
    "    api_key,\n",
    "    model=\"gpt-4o\",\n",
    "    embedding_model=\"text-embedding-3-large\",\n",
    "    index_dir=\"./merging_index\",\n",
    "    chunk_sizes=None\n",
    "):\n",
    "    chunk_sizes = chunk_sizes or [2048, 512, 128]\n",
    "\n",
    "    # Load and parse the document\n",
    "    documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
    "    document_text = \"\\n\\n\".join([doc.text for doc in documents])\n",
    "    document = Document(text=document_text)\n",
    "\n",
    "    # Segment the document using a Hierarchical Node Parser\n",
    "    node_parser = HierarchicalNodeParser.from_defaults(chunk_sizes=chunk_sizes)\n",
    "    nodes = node_parser.get_nodes_from_documents([document])\n",
    "    leaf_nodes = get_leaf_nodes(nodes)\n",
    "\n",
    "    # Set up OpenAI integration and embeddings\n",
    "    openai.api_key = api_key\n",
    "    llm = OpenAI(model=model, temperature=0.1, api_key=api_key)\n",
    "    embedding = OpenAIEmbedding(model=embedding_model)\n",
    "\n",
    "    # Create the index\n",
    "    Settings.llm = llm\n",
    "    Settings.embeddings = embedding\n",
    "\n",
    "    storage_context = StorageContext.from_defaults()\n",
    "    storage_context.docstore.add_documents(nodes)\n",
    "\n",
    "    if not os.path.exists(index_dir):\n",
    "        automerging_index = VectorStoreIndex(\n",
    "            leaf_nodes, storage_context=storage_context, service_context=Settings\n",
    "        )\n",
    "        automerging_index.storage_context.persist(persist_dir=index_dir)\n",
    "    else:\n",
    "        automerging_index = load_index_from_storage(\n",
    "            StorageContext.from_defaults(persist_dir=index_dir),\n",
    "            service_context=Settings\n",
    "        )\n",
    "    return automerging_index\n",
    "\n",
    "\n",
    "# Function to get the automerging query engine.\n",
    "def get_automerging_query_engine(automerge_index, similarity_top_k=12, rerank_top_n=6):\n",
    "    base_retriever = automerge_index.as_retriever(similarity_top_k=similarity_top_k)\n",
    "    retriever = AutoMergingRetriever(\n",
    "        base_retriever, automerge_index.storage_context, verbose=True\n",
    "    )\n",
    "    rerank = SentenceTransformerRerank(\n",
    "        top_n=rerank_top_n, model=\"BAAI/bge-reranker-base\"\n",
    "    )\n",
    "    auto_merging_engine = RetrieverQueryEngine.from_args(\n",
    "        retriever, node_postprocessors=[rerank]\n",
    "    )\n",
    "    return auto_merging_engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** Dataset dependencies:\n",
       "- LAION-5B: Used to establish the PathLAION collection, which contains pathology image–text data from sources beyond Twitter. This subset was used for training the main model.\n",
       "\n",
       "Model dependencies:\n",
       "- CLIP: Fine-tuned to develop the PLIP model for visual–language representation and learning in pathology."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import utils\n",
    "from llama_index.core.response.notebook_utils import display_response\n",
    "\n",
    "api_key = utils.get_openai_api_key()\n",
    "file_path = \"./model-papers/plip.pdf\"\n",
    "\n",
    "# Build or load the sentence window index\n",
    "sentence_index = build_sentence_window_index(file_path, api_key)\n",
    "\n",
    "# Get the query engine\n",
    "sentence_window_engine = get_sentence_window_query_engine(sentence_index)\n",
    "\n",
    "query = \"\"\"\n",
    "    Please extract and list all dataset dependencies and model dependencies mentioned in the research paper that were used for training or fine-tuning the main model\n",
    "\n",
    "    - Include pre-trained models that were fine-tuned or further trained as part of the model development process.\n",
    "    - Exclude all datasets and models used solely for validation, testing, evaluation, baseline comparisons or benchmarking.\n",
    "    - For datasets, if a subset was used, list the original, larger dataset as the dependency.\n",
    "    - Provide a brief explanation for each dependency, showing how it was used in the model development.\n",
    "    - Exclude general concepts, libraries, tools, and architectures (e.g., Scikit-learn, Logistic Regression, Variational Autoencoder, Text Transformer, etc).\n",
    "\n",
    "    For instance, if a paper states 'we fine-tuned a pre-trained Model X', then Model X should be listed as a dependency.\n",
    "\n",
    "    Present the information in this format:\n",
    "    Dataset dependencies:\n",
    "    - [Dataset name]: [Brief explanation of its use in training/fine-tuning]\n",
    "    Model dependencies:\n",
    "    - [Model name]: [Brief explanation of its use in training/fine-tuning]\n",
    "\n",
    "    If no relevant datasets or models are identified, state \"None identified\" under the respective category.\n",
    "    DO NOT include any other information in your response.\n",
    "\"\"\"\n",
    "\n",
    "# Query the index\n",
    "window_response = sentence_window_engine.query(query)\n",
    "display_response(window_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modelrecords",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
