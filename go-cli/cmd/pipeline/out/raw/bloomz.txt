Model Details:
  Name: BLOOMZ & mT0
  Version: "N/A"
  Release Date: "N/A"
  Developed By: BigScience Workshop
  Model Type: Multilingual and Multitask Language Model

Intended Use:
  - Natural language understanding and generation across multiple languages
  - Crosslingual generalization to unseen tasks and languages

Factors:
  - Language
  - Task complexity

Metrics:
  - Accuracy
  - Pass@k rates for Program synthesis

Evaluation Data:
  Description: The evaluation data varies across different tasks including coreference resolution, natural language inference, program synthesis, and sentence completion, with datasets such as Winogrande, XWinograd, ANLI, XNLI, HumanEval, StoryCloze, SuperGLUE, and XCOPA.

Training Data:
  Description: The model was finetuned on the crosslingual task mixture (xP3), which includes data for performing various tasks across multiple languages. For pretraining data, refer to the dataset bigscience/xP3.

Ethical Considerations:
  - Potential for biased outcomes based on the training data
  - Misuse in generating harmful, misleading, or unethical content

Caveats and Recommendations:
  - Prompt engineering is crucial for effective model performance; clear delineation of input and desired output helps.
  - Providing ample context, especially when requesting outputs in specific languages or domains, improves model accuracy.

Additional Information:
  - The BLOOMZ & mT0 family includes models of various sizes, suited for different computational capabilities and task requirements.
  - Performance can significantly vary based on how prompts are structured; experimenting with different prompting strategies is recommended for optimal results.